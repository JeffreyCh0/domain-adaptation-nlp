{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import zipfile\n",
    "import bz2\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = \"../data/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0           1                             2         3                4  \\\n",
       "0  0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  _TheSpecialOne_   \n",
       "1  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY    scotthamilton   \n",
       "2  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY         mattycus   \n",
       "3  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          ElleCTF   \n",
       "4  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY           Karoli   \n",
       "\n",
       "                                                   5  \n",
       "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1  is upset that he can't update his Facebook by ...  \n",
       "2  @Kenichan I dived many times for the ball. Man...  \n",
       "3    my whole body feels itchy and like its on fire   \n",
       "4  @nationwideclass no, it's not behaving at all....  "
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "twitter_zip = zipfile.ZipFile(data_path + \"twitter/twitter.zip\")\n",
    "twitter = pd.read_csv(twitter_zip.open('training.1600000.processed.noemoticon.csv'),encoding='ISO-8859-1', header=None)\n",
    "twitter.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_tw = twitter[0].values\n",
    "y_tw[y_tw==4]=1\n",
    "X_tw = twitter[5].values\n",
    "\n",
    "X_train_val_tw, X_test_tw, y_train_val_tw, y_test_tw = train_test_split(X_tw, y_tw, test_size=0.33, random_state=7)\n",
    "X_train_tw, X_dev_tw, y_train_tw, y_dev_tw = train_test_split(X_train_val_tw, y_train_val_tw, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Amazon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_train_file = bz2.BZ2File(data_path + 'amazon/train.ft.txt.bz2').readlines()\n",
    "amazon_test_file = bz2.BZ2File(data_path + 'amazon/test.ft.txt.bz2').readlines()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_train_file = [x.decode('utf-8') for x in amazon_train_file]\n",
    "amazon_test_file = [x.decode('utf-8') for x in amazon_test_file]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon_train_file[0].find(\" \")\n",
    "def get_label_feature_amazon(file):\n",
    "    labels = []\n",
    "    reviews = []\n",
    "    for i, e in enumerate(file):\n",
    "        sep_pos = e.find(\" \")\n",
    "        label = 1 if e[:sep_pos]==\"__label__2\" else 0\n",
    "        review = e[(sep_pos+1):]\n",
    "        labels.append(label)\n",
    "        reviews.append(review)\n",
    "    return reviews, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_val_az, y_train_val_az = get_label_feature_amazon(amazon_train_file)\n",
    "X_test_az, y_test_az = get_label_feature_amazon(amazon_train_file)\n",
    "X_train_az, X_dev_az, y_train_az, y_dev_az = train_test_split(X_train_val_az, y_train_val_az, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5095"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(y_train_az[:2000])/len(y_train_az[:2000])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Movie"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_zip = zipfile.ZipFile(data_path + \"movies/movie.zip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_train = pd.read_csv(movie_zip.open('Train.csv'))\n",
    "movie_dev = pd.read_csv(movie_zip.open('Valid.csv'))\n",
    "movie_test = pd.read_csv(movie_zip.open('Test.csv'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_mv = movie_train['text'].values\n",
    "X_dev_mv = movie_dev['text'].values\n",
    "X_test_mv = movie_test['text'].values\n",
    "\n",
    "y_train_mv = movie_train['label'].values\n",
    "y_dev_mv = movie_dev['label'].values\n",
    "y_test_mv = movie_test['label'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('I grew up (b. 1965) watching and loving the Thunderbirds. All my mates at school watched. We played \"Thunderbirds\" before school, during lunch and after school. We all wanted to be Virgil or Scott. No one wanted to be Alan. Counting down from 5 became an art form. I took my children to see the movie hoping they would get a glimpse of what I loved as a child. How bitterly disappointing. The only high point was the snappy theme tune. Not that it could compare with the original score of the Thunderbirds. Thankfully early Saturday mornings one television channel still plays reruns of the series Gerry Anderson and his wife created. Jonatha Frakes should hand in his directors chair, his version was completely hopeless. A waste of film. Utter rubbish. A CGI remake may be acceptable but replacing marionettes with Homo sapiens subsp. sapiens was a huge error of judgment.',\n",
       " 0)"
      ]
     },
     "execution_count": 133,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_mv[0], y_train_mv[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Finance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance_file = pd.read_csv(data_path + \"/finance/archive/all-data.csv\", encoding='ISO-8859-1', header = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [],
   "source": [
    "finance =  finance_file[finance_file[0]!=\"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_fi = finance[0].values\n",
    "y_fi[y_fi==\"negative\"] = 0\n",
    "y_fi[y_fi==\"positive\"] = 1\n",
    "y_fi = y_fi.astype(\"int64\")\n",
    "X_fi = finance[1].values\n",
    "\n",
    "X_train_val_fi, X_test_fi, y_train_val_fi, y_test_fi = train_test_split(X_fi, y_fi, test_size=0.33, random_state=7)\n",
    "X_train_fi, X_dev_fi, y_train_fi, y_dev_fi = train_test_split(X_train_val_fi, y_train_val_fi, test_size=0.33, random_state=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Encode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_d = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_d = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['@Jayme1988 well when you are younger Id agree but as we get older..marrying for money is the way to go ',\n",
       " '@Jackalltimelow Awee poor Jack, all alllone! ',\n",
       " 'my tummy hurts. ',\n",
       " 'mmmmm pa says my room has to be blue once again  ',\n",
       " '@xsameehx wish you were here my love. Our superhero trio is not the same as a duo only ',\n",
       " 'Watched War of the Worlds. Not bad. Kept seeing Kathy Bates tits flash in my head. ',\n",
       " 'Just had a really lovely conversation on omegle  I think i made a new friend',\n",
       " \"@tommcfly tom, isn't your fault, you're still the best \",\n",
       " '@catnip070 Caught a cold  cough cough.',\n",
       " \"@blacksocialite I think we staying at slice... I showed 255 my real I'd \"]"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(X_train)[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def tokenize_sentences(tokenizer, input_sentences, output_path):\n",
    "#     encoded_input = tokenizer(input_sentences, return_tensors = 'pt', truncation = True, padding = True)\n",
    "#     torch.save(encoded_input, output_path)\n",
    "#     return encoded_input\n",
    "\n",
    "# def encode_sentences(tokenizer, model, input_sentences, tokenize_path, output_path, batch_size):\n",
    "#     output = np.zeros([len(input_sentences), 768])\n",
    "#     encoded_input = torch.load(tokenize_path)\n",
    "#     with torch.no_grad():\n",
    "#         for i in range(int(len(X_train)/batch_size)):\n",
    "#             encoded_input_batch = {}\n",
    "#             encoded_input_batch['input_ids'] = encoded_input['input_ids'][i*batch_size: min(len(input_sentences), (i+1)*batch_size)]\n",
    "#             encoded_input_batch['attention_mask'] = encoded_input['attention_mask'][i*batch_size: min(len(input_sentences), (i+1)*batch_size)]\n",
    "#             output[i*batch_size: min(len(input_sentences),(i+1)*batch_size)] = model_d(**encoded_input_batch)[0][:,0,:].numpy()\n",
    "#     np.save(output_path, output)\n",
    "#     return output\n",
    "\n",
    "def tokenize_encode_sentences(tokenizer, model, input_sentences, output_path):\n",
    "    output = np.zeros([len(input_sentences), 768])\n",
    "    for i, x in enumerate(input_sentences): \n",
    "        output[i] = tokenize_encode_sentences_sample(tokenizer_d, model_d, [x], output_path)\n",
    "    np.save(output_path, output)\n",
    "    return output\n",
    "\n",
    "def tokenize_encode_sentences_sample(tokenizer, model, input_sentences, output_path):\n",
    "    encoded_input = tokenizer(input_sentences, return_tensors = 'pt', truncation = True, padding = True)\n",
    "    output = model_d(**encoded_input)[0][:,0,:].detach().numpy()\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_tw = tokenize_encode_sentences(tokenizer_d, model_d, list(X_train_tw[:2000]), \"../outputs/\" + \"encoded_twitter_train_2000\")\n",
    "output_dev_tw = tokenize_encode_sentences(tokenizer_d, model_d, list(X_dev_tw[:2000]), \"../outputs/\" + \"encoded_twitter_dev_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_az = tokenize_encode_sentences(tokenizer_d, model_d, list(X_train_az[:2000]), \"../outputs/\" + \"encoded_amazon_train_2000\")\n",
    "output_dev_az = tokenize_encode_sentences(tokenizer_d, model_d, list(X_dev_az[:2000]), \"../outputs/\" + \"encoded_amazon_dev_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_mv = tokenize_encode_sentences(tokenizer_d, model_d, list(X_train_mv[:2000]), \"../outputs/\" + \"encoded_movie_train_2000\")\n",
    "output_dev_mv = tokenize_encode_sentences(tokenizer_d, model_d, list(X_dev_mv[:2000]), \"../outputs/\" + \"encoded_movie_dev_2000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_train_fi = tokenize_encode_sentences(tokenizer_d, model_d, list(X_train_fi[:2000]), \"../outputs/\" + \"encoded_finance_train_2000\")\n",
    "output_dev_fi = tokenize_encode_sentences(tokenizer_d, model_d, list(X_dev_fi[:2000]), \"../outputs/\" + \"encoded_finance_dev_2000\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "lr = LogisticRegression(C=0.1, max_iter = 200000)\n",
    "\n",
    "datasets = [\"tw\", \"az\", \"mv\", \"fi\"]\n",
    "\n",
    "def results(model, datasets):\n",
    "    for s in datasets:\n",
    "        print(s)\n",
    "        model.fit(eval(\"output_train_\" + s), eval(\"y_train_\" + s)[:2000])\n",
    "        s_train = model.score(eval(\"output_train_\" + s), eval(\"y_train_\" + s)[:2000])\n",
    "        s_val = model.score(eval(\"output_dev_\" + s), eval(\"y_dev_\" + s)[:2000])\n",
    "        print(\"source train:\", s_train)\n",
    "        print(\"source val:\", s_val)\n",
    "        datasets_ts = datasets.copy()\n",
    "        datasets_ts.remove(s)\n",
    "        \n",
    "        for t in datasets_ts:\n",
    "            t_val = model.score(eval(\"output_train_\" + t), eval(\"y_train_\" + t)[:2000])\n",
    "            print(\"target \" + t + \":\", t_val)\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tw\n",
      "source train: 0.8\n",
      "source val: 0.751\n",
      "target az: 0.8065\n",
      "target mv: 0.758\n",
      "target fi: 0.7721088435374149\n",
      "\n",
      "az\n",
      "source train: 0.9015\n",
      "source val: 0.884\n",
      "target tw: 0.721\n",
      "target mv: 0.8185\n",
      "target fi: 0.8015873015873016\n",
      "\n",
      "mv\n",
      "source train: 0.87\n",
      "source val: 0.8575\n",
      "target tw: 0.7005\n",
      "target az: 0.8535\n",
      "target fi: 0.8344671201814059\n",
      "\n",
      "fi\n",
      "source train: 0.8820861678004536\n",
      "source val: 0.8344827586206897\n",
      "target tw: 0.711\n",
      "target az: 0.7675\n",
      "target mv: 0.7535\n",
      "\n"
     ]
    }
   ],
   "source": [
    "results(lr, datasets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conclusion:\n",
    "We can use: \n",
    "* tw --> fi\n",
    "* az --> mv\n",
    "* az --> fi\n",
    "* mv --> az\n",
    "* fi --> az (low priority, bc rarely finance is the source in real world)\n",
    "* fi --> mv (low priority, bc rarely finance is the source in real world)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
