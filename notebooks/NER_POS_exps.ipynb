{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "sys.path.append(\"../src\")\n",
    "from bert_embedding import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SEC filings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_link =\"https://github.com/juand-r/entity-recognition-datasets/tree/master/data/SEC-filings/CONLL-format/data/test/FIN3\"\n",
    "train_link = \"https://github.com/juand-r/entity-recognition-datasets/tree/master/data/SEC-filings/CONLL-format/data/train/FIN5.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/SEC-filings/CONLL-format/data/test/FIN3.txt -P ../data/sec_ner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/SEC-filings/CONLL-format/data/train/FIN5.txt -P ../data/sec_ner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. BTC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/BTC/CONLL-format/data/h.conll -P ../data/ner_btc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Wikigold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !wget https://raw.githubusercontent.com/juand-r/entity-recognition-datasets/master/data/wikigold/CONLL-format/data/wikigold.conll.txt -P ../data/ner_wikigold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "def load_ner_data(path, separator = \" \"):\n",
    "    with open(path) as f:\n",
    "        text = f.read().split(\"\\n\\n\")\n",
    "\n",
    "    output = []\n",
    "    for line in text:\n",
    "        feature_label = []\n",
    "        line = line.split(\"\\n\")\n",
    "        for entry in line:\n",
    "            feature_label.append(tuple(entry.split(separator)))\n",
    "        output.append(feature_label)\n",
    "    return output\n",
    "\n",
    "# get words and tags\n",
    "def unique_words_tags(data):\n",
    "    unique_words = []\n",
    "    unique_tags = []\n",
    "    for sent in data:\n",
    "        unique_words.extend(list(set(np.array(sent)[:,0])))\n",
    "        unique_tags.extend(list(set(np.array(sent)[:,-1])))\n",
    "        \n",
    "    return set(unique_words), set(unique_tags)\n",
    "\n",
    "# get words and tags distributions\n",
    "def distributions_words_tags(data):\n",
    "    unique_words = {}\n",
    "    unique_tags = {}\n",
    "    for i in range(len(data)-1):\n",
    "        sent = data[i]\n",
    "        for t in sent:\n",
    "            word = t[0]\n",
    "            tag = t[-1]\n",
    "            \n",
    "            if word in unique_words:\n",
    "                unique_words[word] += 1\n",
    "            else:\n",
    "                unique_words[word] = 1\n",
    "                \n",
    "            if tag in unique_tags:\n",
    "                unique_tags[tag] += 1\n",
    "            else:\n",
    "                unique_tags[tag] = 1\n",
    "                \n",
    "    return unique_words, unique_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['3', '1', '2']"
      ]
     },
     "execution_count": 244,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = []\n",
    "a.extend(list(set([\"1\",\"2\",\"3\"])))\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "sec_path = \"../data/ner_sec/FIN5.txt\"\n",
    "sec = load_ner_data(sec_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1170\n"
     ]
    }
   ],
   "source": [
    "print(len(sec))\n",
    "# sec[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "9345\n"
     ]
    }
   ],
   "source": [
    "btc = []\n",
    "for data in [\"a\", \"b\", \"e\", \"f\", \"g\", \"h\"]:\n",
    "    btc.extend(load_ner_data(\"../data/ner_btc/\" + data + \".conll\", \"\\t\"))\n",
    "print(len(btc))\n",
    "# btc[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1842\n"
     ]
    }
   ],
   "source": [
    "wiki = load_ner_data(\"../data/ner_wikigold/\" + \"wikigold\" + \".conll.txt\", \" \")\n",
    "print(len(wiki))\n",
    "# wiki[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'O'"
      ]
     },
     "execution_count": 236,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki[1][0][-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'I-MISC': 1392, 'O': 32721, 'I-ORG': 1958, 'I-PER': 1634, 'I-LOC': 1447}\n",
      "{'O': 39485, 'I-ORG': 384, 'I-LOC': 356, 'I-PER': 783, 'I-MISC': 7}\n",
      "{'O': 131814, 'B-LOC': 2822, 'B-PER': 7928, 'B-ORG': 4135, 'I-ORG': 1176, 'I-PER': 1554, 'I-LOC': 958, '': 5}\n"
     ]
    }
   ],
   "source": [
    "for data in [wiki, sec, btc]:\n",
    "    print(distributions_words_tags(data)[1])\n",
    "# only wiki and sec works here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('',)]\n"
     ]
    }
   ],
   "source": [
    "for sent in wiki:\n",
    "    for t in sent:\n",
    "        if t[-1] ==\"\":\n",
    "            print(sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 365,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels(sent):\n",
    "    return [label for token, label in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 367,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent2labels1(sent):\n",
    "    return [t[-1] for t in sent]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 369,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'O']"
      ]
     },
     "execution_count": 369,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2labels(wiki[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 370,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-MISC',\n",
       " 'O',\n",
       " 'O',\n",
       " 'O',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'I-ORG',\n",
       " 'O']"
      ]
     },
     "execution_count": 370,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sent2labels1(wiki[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2features(sent, i):\n",
    "    \"\"\"\n",
    "    The function generates all features\n",
    "    for the word at position i in the\n",
    "    sentence.\n",
    "    \"\"\"\n",
    "    word = sent[i][0]\n",
    "    f = tokenize_encode_bert_sentences_sample(tokenizer_d, model_d, word)[0]\n",
    "    features = {}\n",
    "    for j in range(len(f)):\n",
    "        features[str(j)] = f[j]\n",
    "    return features\n",
    "\n",
    "\n",
    "def sent2features(sent):\n",
    "    return [word2features(sent, i) for i in range(len(sent))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [sent2labels1(s) for s in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [sent2features(s) for s in wiki]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features[0:2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 438,
   "metadata": {},
   "outputs": [],
   "source": [
    "haha = tokenize_encode_bert_sentences_sample(tokenizer_d, model_d, \"haha\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 437,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[('/', 'NN', '-', 'O'),\n",
       "  ('s', 'NNS', '-', 'O'),\n",
       "  ('/', ':', '-', 'O'),\n",
       "  ('Bing', 'VBG', '-', 'I-PER'),\n",
       "  ('Yu', 'NNP', '-', 'I-PER')],\n",
       " [('',)]]"
      ]
     },
     "execution_count": 437,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = sec[-2:]\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 442,
   "metadata": {},
   "outputs": [],
   "source": [
    "# haha[0]\n",
    "# haha_f = {}\n",
    "# for j in range(len(haha[0])):\n",
    "#     haha_f[str(j)] = haha[0][j]\n",
    "# haha_f"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://towardsdatascience.com/named-entity-recognition-ner-meeting-industrys-requirement-by-applying-state-of-the-art-deep-698d2b3b4ede\n",
    "https://www.depends-on-the-definition.com/sequence-tagging-lstm-crf/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_d = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_d = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8506 3513 10694\n"
     ]
    }
   ],
   "source": [
    "words_wiki, tags = unique_words_tags(wiki)\n",
    "words_sec, tags = unique_words_tags(sec)\n",
    "print(len(words_wiki), len(words_sec), len(words_wiki | words_sec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 413\n"
     ]
    }
   ],
   "source": [
    "sent_lens = [len(x) for x in wiki]\n",
    "print(max([len(x) for x in wiki]), max([len(x) for x in sec]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 460,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 200\n",
    "words_wiki.remove(\"\")\n",
    "words_sec.remove(\"\")\n",
    "word2idx = {w: i + 1 for i, w in enumerate(words_wiki | words_sec)}\n",
    "tag2idx = {t: i for i, t in enumerate(tags)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [0., 1., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from keras.utils import to_categorical\n",
    "y_wiki = [to_categorical(i, num_classes=len(tags)) for i in y]\n",
    "y_wiki[0][0]\n",
    "\n",
    "y_sec = [to_categorical(i, num_classes=len(tags)) for i in y]\n",
    "y_sec[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 453,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_wiki, test_wiki = train_test_split(wiki)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_ner_corpus = tokenize_encode_bert_sentences(tokenizer_d, model_d, words_list, \"../data/all_bert/encoded_ner_corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 470,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28524143, -0.04883502, -0.08547825, -0.10694244,  0.0716219 ,\n",
       "       -0.06495161,  0.08082949,  0.10530194, -0.15842576, -0.17903239],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 470,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenize_encode_bert_sentences_sample(tokenizer_d, model_d, [\"Randall\"])[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 471,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-0.28524143, -0.04883502, -0.08547825, -0.10694244,  0.0716219 ,\n",
       "       -0.06495161,  0.08082949,  0.10530194, -0.15842576, -0.17903239])"
      ]
     },
     "execution_count": 471,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoded_ner_corpus[0][:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 477,
   "metadata": {},
   "outputs": [],
   "source": [
    "ner_bert = np.load(\"../data/all_bert/encoded_ner_corpus.npy\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
