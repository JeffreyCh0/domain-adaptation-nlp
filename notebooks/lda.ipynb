{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follows tutorial here:\n",
    "https://towardsdatascience.com/topic-modeling-and-latent-dirichlet-allocation-in-python-9bf156893c24"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import re\n",
    "from os import walk\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a.split(\"\\t\")\n",
    "data_path = '../data/amazon_reviews/amazon_review/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(filepath):\n",
    "    with open(filepath, \"r\") as file:\n",
    "        text = file.read()\n",
    "    reviews = text.split(\"\\n\")\n",
    "    reviews = [i for i in reviews if (i != \"\") and (\"UNCONFIDENT_INTENT_FROM_SLAD\" not in i)]\n",
    "    x = [x.split(\"\\t\")[0] for x in reviews]\n",
    "    y = [x.split(\"\\t\")[1] for x in reviews]\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1', '2'}"
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(load_data(data_path + \"test/\" + \"Baby.test\")[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = {}\n",
    "for domain in [\"train\", \"dev\", \"test\"]:\n",
    "    folder_path = data_path + domain + \"/\"\n",
    "    onlyfiles = [f for f in listdir(folder_path) if isfile(join(folder_path, f))]\n",
    "    for file in onlyfiles:\n",
    "        if (\".test\" in file) or (\".train\" in file) or (\".test\" in file):\n",
    "            all_data[file] = load_data(folder_path + \"/\" + file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['CDs_and_Vinyl.train', 'Clothing_Shoes_and_Jewelry.train', 'Home_and_Kitchen.train', 'Beauty.train', 'Sports_and_Outdoors.train', 'Movies_and_TV.train', 'Apps_for_Android.train', 'Cell_Phones_and_Accessories.train', 'Electronics.train', 'Office_Products.train', 'Books.train', 'Health_and_Personal_Care.train', 'Kindle_Store.train', 'Grocery_and_Gourmet_Food.train', 'Pet_Supplies.train', 'Tools_and_Home_Improvement.test', 'Tools_and_Home_Improvement.train', 'Pet_Supplies.test', 'Automotive.test', 'Automotive.train', 'Grocery_and_Gourmet_Food.test', 'Baby.test', 'Video_Games.train', 'Baby.train', 'Digital_Music.test', 'Digital_Music.train', 'Toys_and_Games.train', 'Toys_and_Games.test', 'Video_Games.test'])"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_data.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_cleaned/amazon_data_dict.txt\",\"w\") as f:\n",
    "    json.dump(all_data, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/yuchen.zhang/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.utils import simple_preprocess\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from nltk.stem import WordNetLemmatizer, SnowballStemmer\n",
    "from nltk.stem.porter import *\n",
    "from nltk.stem import *\n",
    "import numpy as np\n",
    "np.random.seed(2021)\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_used = ['Home_and_Kitchen', 'Books', 'Electronics', 'Movies_and_TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "def lemmatize_stemming(text):\n",
    "    return stemmer.stem(WordNetLemmatizer().lemmatize(text, pos='v'))\n",
    "\n",
    "def preprocess(text):\n",
    "    result = []\n",
    "    for token in gensim.utils.simple_preprocess(text):\n",
    "        if token not in gensim.parsing.preprocessing.STOPWORDS and len(token) > 3:\n",
    "            result.append(lemmatize_stemming(token))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'1'"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Electronics'][1][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['check', 'barn', 'nobl', 'number', 'buyaccessori', 'pay', 'day']"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(data['Electronics'][0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Check your SN with Barnes & Noble's 800 number. Mine was used from BuyAccessories. I paid $70 too much as used is under $30 these days.\""
      ]
     },
     "execution_count": 168,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['Electronics'][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics = {}\n",
    "for i in freq_used:\n",
    "    processed_train = [preprocess(x) for x in all_data[i + '.train'][0]]\n",
    "    topics[i] = processed_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['Home_and_Kitchen', 'Books', 'Electronics', 'Movies_and_TV'])"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "# topics['Home_and_Kitchen']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_cleaned/amazon_topics_dict.txt\",\"w\") as f:\n",
    "    json.dump(topics, f, indent=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append(\"../\")\n",
    "from src.utils import *\n",
    "from src.bert_embedding import *\n",
    "from transformers import BertTokenizer, BertModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_cleaned/amazon_topics_dict.txt\",\"r\") as f:\n",
    "    topics = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../data/all_cleaned/amazon_data_dict.txt\",\"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer_d = DistilBertTokenizer.from_pretrained('distilbert-base-uncased')\n",
    "model_d = DistilBertModel.from_pretrained('distilbert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d77342068b421e945da8cb8dc6c958",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value='Downloading'), FloatProgress(value=0.0, max=440473133.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "model = BertModel.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['receiv', 'piec', 'measur', 'return', 'piec', 'order', 'differ']"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics[\"Home_and_Kitchen\"][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Only received one piece, the 1/4 cup. WHERE ARE THE OTHER FOUR MEASURER'S ??I will be returning the one piece. I already ordered a different set.\""
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"Home_and_Kitchen\"][0][100]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer_d(topics[\"Home_and_Kitchen\"][0], return_tensors='pt', truncation=True, padding=True)\n",
    "encoded_sent = tokenizer_d([\" \".join(topics[\"Home_and_Kitchen\"][0])], return_tensors='pt', truncation=True, padding=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = model_d(**encoded_input)\n",
    "output_sent = model_d(**encoded_sent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2199, -0.1201, -0.1294,  ..., -0.0373,  0.1549,  0.3760],\n",
       "         [ 0.1758, -0.6426,  0.0107,  ...,  0.0864,  0.3222,  0.1949],\n",
       "         [-0.3781, -0.4156,  0.0202,  ...,  0.0596, -0.1450, -0.1192],\n",
       "         [-0.2415, -0.4868, -0.4642,  ...,  0.2593, -0.1489,  0.2280],\n",
       "         [ 0.9567,  0.0983, -0.3869,  ...,  0.2064, -0.7347, -0.0875]],\n",
       "\n",
       "        [[-0.2137, -0.1053, -0.0130,  ..., -0.1954,  0.0230,  0.2719],\n",
       "         [ 0.5586, -0.1439, -0.1018,  ...,  0.0750, -0.0413,  0.2260],\n",
       "         [ 0.9042,  0.2337, -0.3188,  ...,  0.2038, -0.6615, -0.1966],\n",
       "         [ 0.0075,  0.1224,  0.0685,  ...,  0.0251, -0.1264,  0.1510],\n",
       "         [ 0.0734,  0.0553, -0.0359,  ...,  0.0413, -0.1558,  0.2330]],\n",
       "\n",
       "        [[-0.3395,  0.0194, -0.1812,  ..., -0.1555,  0.2767,  0.2707],\n",
       "         [-0.1405,  0.0508, -0.0577,  ..., -0.3683,  0.5464,  0.0723],\n",
       "         [-0.1438, -0.1855,  0.2111,  ..., -0.2795,  0.1649, -0.3668],\n",
       "         [-0.2358, -0.4288, -0.7102,  ..., -0.1054,  0.2284, -0.0944],\n",
       "         [ 0.8653,  0.1167, -0.3814,  ...,  0.1156, -0.6329, -0.1220]],\n",
       "\n",
       "        ...,\n",
       "\n",
       "        [[-0.1352, -0.1746, -0.0476,  ..., -0.1152,  0.0502,  0.3989],\n",
       "         [ 0.2233, -0.0603, -0.2780,  ..., -0.1215,  0.2894,  0.2752],\n",
       "         [ 0.8735,  0.0475, -0.3427,  ...,  0.1884, -0.7546, -0.2225],\n",
       "         [ 0.0492, -0.1938,  0.1318,  ...,  0.0450, -0.0486,  0.2725],\n",
       "         [ 0.1007, -0.2394,  0.0291,  ...,  0.0556, -0.1062,  0.3264]],\n",
       "\n",
       "        [[-0.1918,  0.0133, -0.0405,  ..., -0.0639,  0.2036,  0.3877],\n",
       "         [-0.0161,  0.0061, -0.2482,  ..., -0.1297,  0.6132,  0.5495],\n",
       "         [ 0.1434,  0.2262, -0.1323,  ..., -0.2178,  0.0042,  0.0058],\n",
       "         [ 1.0124,  0.3010, -0.3376,  ...,  0.1673, -0.6818, -0.0585],\n",
       "         [ 0.0662, -0.2125,  0.0176,  ..., -0.1290,  0.1346,  0.3554]],\n",
       "\n",
       "        [[-0.1347, -0.1744,  0.0466,  ..., -0.0854,  0.0351,  0.1978],\n",
       "         [ 0.3645, -0.1643, -0.1346,  ...,  0.0374,  0.0356,  0.2044],\n",
       "         [ 0.8980,  0.1152, -0.2950,  ...,  0.1795, -0.7341, -0.2593],\n",
       "         [ 0.0901, -0.0752,  0.3029,  ...,  0.0403, -0.0266,  0.1512],\n",
       "         [ 0.0882, -0.1873,  0.2218,  ..., -0.0276, -0.1083,  0.2351]]],\n",
       "       grad_fn=<NativeLayerNormBackward>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output.last_hidden_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 5, 768])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 14, 768])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_sent[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "piec trash crush motor sound like explod frustrat tri smoothi replac oster beehiv classic better actual quieter sturdi base\n",
      "A piece of trash for ice crushing.  Motor sounds like it's about to explode. The cup is just frustration if you're trying to make smoothies. Replaced it with the Oster Beehive classic. Way better and actually quieter because of the sturdy base.\n"
     ]
    }
   ],
   "source": [
    "n = 10\n",
    "print(\" \".join(topics[\"Home_and_Kitchen\"][10]))\n",
    "print(data[\"Home_and_Kitchen\"][0][10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_only = \" \".join(topics[\"Home_and_Kitchen\"][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'remedi wilton magazin money idea instruct invest'"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topics_only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_only_shuffle = 'remedi magazin wilton money idea instruct invest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = data[\"Home_and_Kitchen\"][0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input_shuffle = tokenizer(topics_only_shuffle, return_tensors='pt', truncation=True, padding=True)\n",
    "output_shuffle= model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input = tokenizer(topics_only, return_tensors='pt', truncation=True, padding=True)\n",
    "output = model(**encoded_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoded_input_all = tokenizer(all_data, return_tensors='pt', truncation=True, padding=True)\n",
    "output_all = model(**encoded_input_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert = output[0][:, 0, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_all = output_all[0][:, 0, :].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[ 0.14011003,  0.02450287,  0.04106272, ..., -0.5889588 ,\n",
       "          0.15424612,  0.75548756],\n",
       "        [-0.42383468, -0.16127616, -0.06394659, ..., -0.64132077,\n",
       "          1.0293195 ,  0.34360573],\n",
       "        [-0.1286612 ,  0.16680661,  0.5434309 , ..., -0.666994  ,\n",
       "          0.29372048,  0.6447649 ],\n",
       "        ...,\n",
       "        [ 1.0262218 , -0.21811685, -0.45390967, ..., -0.7425849 ,\n",
       "          0.3056906 ,  0.2396598 ],\n",
       "        [ 0.6048785 ,  0.29522845, -0.29891792, ...,  0.0361919 ,\n",
       "         -0.27819633, -0.3662661 ],\n",
       "        [ 0.85183954,  0.38817424, -0.1411262 , ..., -0.16613567,\n",
       "         -0.40873182, -0.27126873]]], dtype=float32)"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_all[0].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_shuffle = output_shuffle[0][:,0,:].detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.77939427]], dtype=float32)"
      ]
     },
     "execution_count": 131,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert @ bert_all.T / (np.linalg.norm(bert) * np.linalg.norm(bert_all))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "freq_used = ['Home_and_Kitchen', 'Books', 'Electronics', 'Movies_and_TV']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_embedd = tokenize_encode_bert_sentences_sample(tokenizer, model, freq_used)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cos_sim(x1,x2):\n",
    "    sim = x1 @ x2.T / (np.linalg.norm(x1) * np.linalg.norm(x2))\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {},
   "outputs": [],
   "source": [
    "sims = {}\n",
    "for i, embeddi in enumerate(topic_embedd):\n",
    "    for j, embeddj in enumerate(topic_embedd):\n",
    "        name = freq_used[i] + \" - \" + freq_used[j]\n",
    "        sims[name] = cos_sim(embeddi, embeddj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Home_and_Kitchen - Home_and_Kitchen': 0.99999994,\n",
       " 'Home_and_Kitchen - Books': 0.8122904,\n",
       " 'Home_and_Kitchen - Electronics': 0.7812052,\n",
       " 'Home_and_Kitchen - Movies_and_TV': 0.8945177,\n",
       " 'Books - Home_and_Kitchen': 0.8122904,\n",
       " 'Books - Books': 0.99999994,\n",
       " 'Books - Electronics': 0.7863649,\n",
       " 'Books - Movies_and_TV': 0.8110619,\n",
       " 'Electronics - Home_and_Kitchen': 0.7812052,\n",
       " 'Electronics - Books': 0.7863649,\n",
       " 'Electronics - Electronics': 1.0,\n",
       " 'Electronics - Movies_and_TV': 0.7939116,\n",
       " 'Movies_and_TV - Home_and_Kitchen': 0.8945177,\n",
       " 'Movies_and_TV - Books': 0.8110619,\n",
       " 'Movies_and_TV - Electronics': 0.7939116,\n",
       " 'Movies_and_TV - Movies_and_TV': 1.0}"
      ]
     },
     "execution_count": 148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "for i in data:\n",
    "    all_data.extend(data[i][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_of_list_of_tokens = [x.split() for x in all_data][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from gensim import corpora, models\n",
    "\n",
    "# # list_of_list_of_tokens = [[\"a\",\"b\",\"c\"], [\"d\",\"e\",\"f\"]]\n",
    "# # [\"a\",\"b\",\"c\"] are the tokens of document 1, [\"d\",\"e\",\"f\"] are the tokens of document 2...\n",
    "# dictionary_LDA = corpora.Dictionary(list_of_list_of_tokens)\n",
    "# dictionary_LDA.filter_extremes(no_below=3)\n",
    "# corpus = [dictionary_LDA.doc2bow(list_of_tokens) for list_of_tokens in list_of_list_of_tokens]\n",
    "\n",
    "# num_topics = 50\n",
    "# %time lda_model = models.LdaModel(corpus, num_topics=num_topics, \\\n",
    "#                                   id2word=dictionary_LDA, \\\n",
    "#                                   passes=4, alpha=[0.01]*num_topics, \\\n",
    "#                                   eta=[0.01]*len(dictionary_LDA.keys()))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
