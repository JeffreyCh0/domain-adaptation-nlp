{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "if (!(\"Notification\" in window)) {\n",
       "    alert(\"This browser does not support desktop notifications, so the %%notify magic will not work.\");\n",
       "} else if (Notification.permission !== 'granted' && Notification.permission !== 'denied') {\n",
       "    Notification.requestPermission(function (permission) {\n",
       "        if(!('permission' in Notification)) {\n",
       "            Notification.permission = permission;\n",
       "        }\n",
       "    })\n",
       "}\n"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext jupyternotify"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import linear_model\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, train_test_split, cross_val_score\n",
    "from sklearn.metrics.pairwise import cosine_distances\n",
    "from numpy import dot\n",
    "from numpy.linalg import norm\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "from os import walk\n",
    "from itertools import permutations, combinations\n",
    "import pickle\n",
    "import time\n",
    "import statsmodels.api as sm\n",
    "from scipy import stats\n",
    "from scipy.stats import ttest_ind, linregress\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.manifold import TSNE\n",
    "\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data2df(PATH):\n",
    "    lines = open(PATH, encoding='iso-8859-1').read().strip().split('\\n')\n",
    "    lines = [line.split(\"\\t\") for line in lines]\n",
    "    lines = pd.DataFrame(lines)\n",
    "    lines = lines[lines[1] != 'UNCONFIDENT_INTENT_FROM_SLAD']\n",
    "    lines[1] = lines[1].apply(lambda x: int(x)-1)\n",
    "    return lines\n",
    "\n",
    "def df2embd(df):\n",
    "    max_row = min(len(df[df[1]==1]),len(df[df[1]==0]),1000)\n",
    "    batch_1 = pd.concat((df[df[1]==1].sample(n=max_row, random_state=1),df[df[1]==0].sample(n=max_row, random_state=1)))\n",
    "    tokenized = batch_1[0].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "    max_len = 0\n",
    "    for i in tokenized.values:\n",
    "        if len(i) > max_len:\n",
    "            max_len = len(i)\n",
    "\n",
    "    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "    attention_mask = np.where(padded != 0, 1, 0)\n",
    "    attention_mask.shape\n",
    "    input_ids = torch.tensor(padded)  \n",
    "    attention_mask = torch.tensor(attention_mask)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "    features = last_hidden_states[0][:,0,:].numpy()\n",
    "    labels = batch_1[1]\n",
    "    return features, np.array(labels)\n",
    "\n",
    "def cos_dist(A, B):\n",
    "       return 1 - (dot(A, B)/(norm(A)*norm(B)))\n",
    "\n",
    "def S2T(train_features, train_labels, test_features, test_labels):\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    return lr_clf.score(test_features, test_labels)\n",
    "\n",
    "\n",
    "def S2ti(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "    \n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "        y_pred_store += y_pred.tolist()\n",
    "        y_test_store += y_test\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "def S2t1(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "    \n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = [test_features[i] for i,x in targets[0]]\n",
    "    y_test = [test_labels[i] for i,x in targets[0]]\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "    X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "    y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    output_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    St1_score = sum(output_score)/len(output_score)    \n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, St1_score, distribution\n",
    "    else:\n",
    "        return original_score, St1_score\n",
    "    \n",
    "\n",
    "\n",
    "def si2ti(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "\n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = X_train[len(X_test):]\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = y_train[len(y_pred):]\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "        y_pred_store += y_pred.tolist()\n",
    "        y_test_store += y_test\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "    \n",
    "    \n",
    "def si2ti_1by1(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    while len(target_distances)>0:\n",
    "        X_test = [test_features[target_distances[0][0]]]\n",
    "        y_test = [test_labels[target_distances[0][0]]]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = X_train[len(X_test):]\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = y_train[len(y_pred):]\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "        y_pred_store += y_pred.tolist()\n",
    "        y_test_store += y_test\n",
    "        target_distances = target_distances[1:]\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "def all_combination_test(func,num_i, include_S2T = True, dist_eval = False):\n",
    "    data_permu = list(permutations(all_data,2))\n",
    "    S2T_scores = []\n",
    "    func_scores = []\n",
    "    dist_list = []\n",
    "    for index,permu in enumerate(data_permu):\n",
    "        try:\n",
    "            if dist_eval:\n",
    "                S2T_scr, func_scr, dist = func(permu[0][0], permu[0][1], permu[1][0], permu[1][1],num_i,dist_eval = dist_eval)\n",
    "                dist_list.append(dist)\n",
    "            else:\n",
    "                S2T_scr, func_scr = func(permu[0][0], permu[0][1], permu[1][0], permu[1][1],num_i,dist_eval = dist_eval)\n",
    "            S2T_scores.append(S2T_scr)\n",
    "            func_scores.append(func_scr)\n",
    "            print(index)\n",
    "        except Exception as e:\n",
    "            print('error:',permu[0][2],permu[1][2],e)\n",
    "            S2T_scores.append(999)\n",
    "            func_scores.append(999)\n",
    "    \n",
    "    iterative_scores = [func_scores[i] - S2T_scores[i] for i in range(len(func_scores))]\n",
    "    \n",
    "    if dist_eval:\n",
    "        return iterative_scores,S2T_scores,func_scores, dist_list\n",
    "    else:\n",
    "        return iterative_scores,S2T_scores,func_scores\n",
    "    \n",
    "\n",
    "def st_tf_distance(domain1, domain2):\n",
    "    S_T = [domain1[0][i] for i,val in enumerate(domain1[1]) if val == 1]\n",
    "    S_F = [domain1[0][i] for i,val in enumerate(domain1[1]) if val == 0]\n",
    "    T_T = [domain2[0][i] for i,val in enumerate(domain2[1]) if val == 1]\n",
    "    T_F = [domain2[0][i] for i,val in enumerate(domain2[1]) if val == 0]\n",
    "    S_T_center = np.mean(S_T,axis = 0)\n",
    "    S_F_center = np.mean(S_F,axis = 0)\n",
    "    T_T_center = np.mean(T_T,axis = 0)\n",
    "    T_F_center = np.mean(T_F,axis = 0)\n",
    "    combi_TF = list(combinations([(S_T_center,'S_T'),(S_F_center,'S_F'),(T_T_center,'T_T'),(T_F_center,'T_F')],2))\n",
    "    combi_TF_dist = [(cos_dist(d1[0], d2[0]),d1[1], d2[1]) for d1,d2 in combi_TF]\n",
    "    combi_TF_dist.append((cos_dist(np.mean(domain1[0],axis = 0), np.mean(domain2[0],axis = 0)),'S','T'))\n",
    "    return combi_TF_dist\n",
    "\n",
    "\n",
    "def S2ti_lastModel(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "    \n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    \n",
    "    output_score = [test_labels[i]==y_pred[i] for i in range(len(y_pred))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "\n",
    "def si2ti_lastModel(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "\n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = X_train[len(X_test):]\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = y_train[len(y_pred):]\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    \n",
    "    output_score = [test_labels[i]==y_pred[i] for i in range(len(y_pred))]\n",
    "    \n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "    \n",
    "def S2ti_true(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 1],axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "    \n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "        y_pred_store += y_pred.tolist()\n",
    "        y_test_store += y_test\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "\n",
    "def S2ti_false(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 0],axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "    \n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "        y_pred_store += y_pred.tolist()\n",
    "        y_test_store += y_test\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "def si2ti_lastModel_true(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 1],axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    if eval_bool:\n",
    "        print(\"train.shape: \",train_labels.shape)\n",
    "        print(\"test.shape: \",test_labels.shape) \n",
    "        print(\"histogram of distance:\")\n",
    "        plt.hist(target_distances)\n",
    "        plt.show()\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    targets = []\n",
    "    targets.append(target_distances[:threshold])\n",
    "    for i in range(1,num_i-1):\n",
    "        targets.append(target_distances[i*threshold:(i+1)*threshold])\n",
    "    targets.append(target_distances[(num_i-1)*threshold:])\n",
    "    \n",
    "    # calculate influence of distribution \n",
    "    if dist_eval:\n",
    "        targets_center = []\n",
    "        distribution = []\n",
    "        for target in targets:\n",
    "            X_test = [test_features[i] for i,x in target]\n",
    "            targets_center.append(np.mean(X_test,axis = 0))\n",
    "        distribution.append(cos_dist(source_center, targets_center[0]))\n",
    "        for i in range(len(targets_center)-1):\n",
    "            distribution.append(cos_dist(targets_center[i], targets_center[i+1]))\n",
    "\n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    for target in targets:\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_train = X_train[len(X_test):]\n",
    "        X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "        y_train = y_train[len(y_pred):]\n",
    "        y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    \n",
    "    output_score = [test_labels[i]==y_pred[i] for i in range(len(y_pred))]\n",
    "    \n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "def si2ti_prob(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    conf = 0.1\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(target_distances)>0:\n",
    "        threshold = min(int(len(test_labels)/num_i),len(target_distances))\n",
    "        target = target_distances[:threshold]\n",
    "        target_distances = target_distances[threshold:]\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        X_test_keep = [X_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        y_pred_keep = [y_pred[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        if len(y_pred_keep) != 0:         \n",
    "            X_train = X_train[len(X_test_keep):]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = y_train[len(y_pred_keep):]\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            return_target = [target[i] for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            if len(return_target) == len(previous_r_target):\n",
    "                break\n",
    "            previous_r_target = return_target[:]\n",
    "            target_distances = return_target + target_distances\n",
    "        else:\n",
    "            X_train = X_train[len(X_test):]\n",
    "            X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "            y_train = y_train[len(y_pred):]\n",
    "            y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "            y_pred_store += y_pred.tolist()\n",
    "            y_test_store += y_test\n",
    "        \n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "def si2ti_prob_2(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    conf = 0.2\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(target_distances)>0:\n",
    "        threshold = min(int(len(test_labels)/num_i),len(target_distances))\n",
    "        target = target_distances[:threshold]\n",
    "        target_distances = target_distances[threshold:]\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        X_test_keep = [X_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        y_pred_keep = [y_pred[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        if len(y_pred_keep) != 0:         \n",
    "            X_train = X_train[len(X_test_keep):]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = y_train[len(y_pred_keep):]\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            return_target = [target[i] for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            if len(return_target) == len(previous_r_target):\n",
    "                break\n",
    "            previous_r_target = return_target[:]\n",
    "            target_distances = return_target + target_distances\n",
    "        else:\n",
    "            X_train = X_train[len(X_test):]\n",
    "            X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "            y_train = y_train[len(y_pred):]\n",
    "            y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "            y_pred_store += y_pred.tolist()\n",
    "            y_test_store += y_test\n",
    "        \n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "def si2ti_prob_3(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    conf = 0.3\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(target_distances)>0:\n",
    "        threshold = min(int(len(test_labels)/num_i),len(target_distances))\n",
    "        target = target_distances[:threshold]\n",
    "        target_distances = target_distances[threshold:]\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        X_test_keep = [X_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        y_pred_keep = [y_pred[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        if len(y_pred_keep) != 0:         \n",
    "            X_train = X_train[len(X_test_keep):]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = y_train[len(y_pred_keep):]\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            return_target = [target[i] for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            if len(return_target) == len(previous_r_target):\n",
    "                break\n",
    "            previous_r_target = return_target[:]\n",
    "            target_distances = return_target + target_distances\n",
    "        else:\n",
    "            X_train = X_train[len(X_test):]\n",
    "            X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "            y_train = y_train[len(y_pred):]\n",
    "            y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "            y_pred_store += y_pred.tolist()\n",
    "            y_test_store += y_test\n",
    "        \n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "    \n",
    "def si2ti_prob_4(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    conf = 0.4\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(target_distances)>0:\n",
    "        threshold = min(int(len(test_labels)/num_i),len(target_distances))\n",
    "        target = target_distances[:threshold]\n",
    "        target_distances = target_distances[threshold:]\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        X_test_keep = [X_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        y_pred_keep = [y_pred[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "        if len(y_pred_keep) != 0:         \n",
    "            X_train = X_train[len(X_test_keep):]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = y_train[len(y_pred_keep):]\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            return_target = [target[i] for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            if len(return_target) == len(previous_r_target):\n",
    "                break\n",
    "            previous_r_target = return_target[:]\n",
    "            target_distances = return_target + target_distances\n",
    "        else:\n",
    "            X_train = X_train[len(X_test):]\n",
    "            X_train = np.concatenate((X_train, X_test), axis=0)\n",
    "            y_train = y_train[len(y_pred):]\n",
    "            y_train = np.concatenate((y_train, y_pred), axis=0)\n",
    "            y_pred_store += y_pred.tolist()\n",
    "            y_test_store += y_test\n",
    "        \n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "def upperbound(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    X_test = test_features\n",
    "    y_test = test_labels\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_test, y_test)\n",
    "    y_pred = lr_clf.predict(X_test)\n",
    "    \n",
    "    output_score = [y_pred[i]==y_test[i] for i in range(len(y_pred))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "\n",
    "\n",
    "def si2ti_prob_4_adj(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "  \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(target_distances)>0:\n",
    "        threshold = min(int(len(test_labels)/num_i),len(target_distances))\n",
    "        target = target_distances[:threshold]\n",
    "        target_distances = target_distances[threshold:]\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = X_train[len(X_test_keep):]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = y_train[len(y_pred_keep):]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        return_target = [target[i] for i in not_keep_index]\n",
    "        if len(return_target) == len(previous_r_target):\n",
    "            break\n",
    "        previous_r_target = return_target[:]\n",
    "        target_distances = return_target + target_distances\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "def si2ti_prob_4_adj_lm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    target_distances = [cos_dist(source_center, x) for x in test_features]\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    \n",
    "    target_distances = [(i,x) for i,x in enumerate(target_distances)]\n",
    "    target_distances = sorted(target_distances, key=lambda x: x[1])\n",
    "    \n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # dividing data\n",
    "    threshold = int(len(target_distances)/num_i)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(target_distances)]]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(target_distances)>0:\n",
    "        threshold = min(int(len(test_labels)/num_i),len(target_distances))\n",
    "        target = target_distances[:threshold]\n",
    "        target_distances = target_distances[threshold:]\n",
    "        X_test = [test_features[i] for i,x in target]\n",
    "        y_test = [test_labels[i] for i,x in target]\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = X_train[len(X_test_keep):]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = y_train[len(y_pred_keep):]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        return_target = [target[i] for i in not_keep_index]\n",
    "        if len(return_target) == len(previous_r_target):\n",
    "            break\n",
    "        previous_r_target = return_target[:]\n",
    "        target_distances = return_target + target_distances\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    output_score = [y_pred[i]==test_labels[i] for i in range(len(y_pred))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "    \n",
    "def si2T_prob_4_adj(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(test_labels)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(test_labels)]]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = X_train[len(X_test_keep):]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = y_train[len(y_pred_keep):]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def si2T_prob_4_adj_rand(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = X_train[len(X_test_keep):]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = y_train[len(y_pred_keep):]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return original_score, gradual_score, distribution\n",
    "    else:\n",
    "        return original_score, gradual_score\n",
    "    \n",
    "    \n",
    "def S2T_prob_4_adj(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    \n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "    \n",
    "def si2T_prob_4_adj_rev(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    source_distances = [cos_dist(target_center, x) for x in train_features]\n",
    "    source_distances = [(i,x) for i,x in enumerate(source_distances)]\n",
    "    source_distances = sorted(source_distances, key=lambda x: x[1])\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = [train_features[i] for i,x in source_distances[:len(test_labels)]]\n",
    "    y_train = [train_labels[i] for i,x in source_distances[:len(test_labels)]]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = X_train[len(X_test_keep):]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = y_train[len(y_pred_keep):]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "\n",
    "def S2T_prob_4_adj_cycle_dl(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo = X_pseudo + X_test_keep\n",
    "        y_pseudo = y_pseudo + y_pred_keep\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = abs(y_train_prob - 0.5)\n",
    "        y_train_prob = [(i,prob) for i,prob in enumerate(y_train_prob)]\n",
    "        y_train_prob = sorted(y_train_prob, key=lambda x: x[1])\n",
    "        train_not_keep_index = [i for i,val in y_train_prob[:len(keep_index)]]\n",
    "        len_init_X = len(X_train)\n",
    "        X_train = [X_train[i] for i in range(len_init_X) if i not in train_not_keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len_init_X) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    \n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_prob_4_adj_cycle_dm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo = X_pseudo + X_test_keep\n",
    "        y_pseudo = y_pseudo + y_pred_keep\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = abs(y_train_prob - 0.5)\n",
    "        y_train_prob = [(i,prob) for i,prob in enumerate(y_train_prob)]\n",
    "        y_train_prob = sorted(y_train_prob, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index = [i for i,val in y_train_prob[:len(keep_index)]]\n",
    "        len_init_X = len(X_train)\n",
    "        X_train = [X_train[i] for i in range(len_init_X) if i not in train_not_keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len_init_X) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    \n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "\n",
    "def S2T_prob_4_adj_cycle_dms(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    X_train_source = train_features[:]\n",
    "    y_train_source = train_labels[:]\n",
    "    X_train_target = []\n",
    "    y_train_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        keep_index = []\n",
    "        conf = 0.4\n",
    "        while len(keep_index) == 0 and conf >=0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob) if (val >= 0.5 + conf) or (val < 0.5 - conf)]\n",
    "            not_keep_index = [i for i,val in enumerate(y_prob) if (val < 0.5 + conf) and (val >= 0.5 - conf)]\n",
    "            conf = conf - 0.01\n",
    "        if len(keep_index) == 0:\n",
    "            keep_index = [i for i,val in enumerate(y_prob)]\n",
    "            not_keep_index = []\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo = X_pseudo + X_test_keep\n",
    "        y_pseudo = y_pseudo + y_pred_keep\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train_source)[:, 0]\n",
    "        y_train_prob = abs(y_train_prob - 0.5)\n",
    "        y_train_prob = [(i,prob) for i,prob in enumerate(y_train_prob)]\n",
    "        y_train_prob = sorted(y_train_prob, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index = [i for i,val in y_train_prob[:len(keep_index)]]\n",
    "        len_init_X = len(X_train_source)\n",
    "        X_train_source = [X_train_source[i] for i in range(len_init_X) if i not in train_not_keep_index]\n",
    "        X_train_target = X_train_target + X_test_keep\n",
    "        X_train = X_train_source + X_train_target\n",
    "        y_train_source = [y_train_source[i] for i in range(len_init_X) if i not in train_not_keep_index]\n",
    "        y_train_target = y_train_target + y_pred_keep\n",
    "        y_train = y_train_source + y_train_target\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    \n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "\n",
    "def S2T_p4_adj_blc(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    #######################################\n",
    "    ###########    Standard        ########\n",
    "    #######################################\n",
    "    top_n = num_i\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p4_adj_Xblc(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i*2\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [abs(val-0.5) for val in y_prob]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob = sorted(y_prob, key=lambda x: x[1],reverse = True)\n",
    "        keep_index = [val[0] for val in y_prob[:top_n]]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p4_adj_blcTm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = 100\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    X_balance = train_features[:]\n",
    "    y_balance = train_labels[:]\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_balance, y_balance)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [abs(val-0.5) for val in y_prob]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob = sorted(y_prob, key=lambda x: x[1],reverse = True)\n",
    "        keep_index = [val[0] for val in y_prob[:top_n]]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "        \n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1])\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1],reverse = True)\n",
    "#         y_train_prob = sorted(y_train_prob, key=lambda x: x[1])\n",
    "        n_pos = sum(y_train)\n",
    "        n_neg = len(y_train) - sum(y_train)\n",
    "        if n_pos < n_neg: # if positive dominates\n",
    "            X_balance = np.concatenate((X_train, [X_train[val[0]] for val in y_train_prob_P[:(n_neg-n_pos)]]), axis=0)\n",
    "            y_balance = np.concatenate((y_train, [y_train[val[0]] for val in y_train_prob_P[:(n_neg-n_pos)]]), axis=0)\n",
    "        elif n_pos > n_neg:\n",
    "            X_balance = np.concatenate((X_train,[X_train[val[0]] for val in y_train_prob_N[:(n_pos-n_neg)]]), axis=0)\n",
    "            y_balance = np.concatenate((y_train,[y_train[val[0]] for val in y_train_prob_N[:(n_pos-n_neg)]]), axis=0)\n",
    "        \n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p4_adj_blcTl(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = 20\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    X_balance = train_features[:]\n",
    "    y_balance = train_labels[:]\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_balance, y_balance)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [abs(val-0.5) for val in y_prob]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob = sorted(y_prob, key=lambda x: x[1],reverse = True)\n",
    "        keep_index = [val[0] for val in y_prob[:top_n]]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "        \n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1],reverse = True)\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1])\n",
    "#         y_train_prob = sorted(y_train_prob, key=lambda x: x[1])\n",
    "        n_pos = sum(y_train)\n",
    "        n_neg = len(y_train) - sum(y_train)\n",
    "        if n_pos < n_neg: # if positive dominates\n",
    "            X_balance = np.concatenate((X_train, [X_train[val[0]] for val in y_train_prob_P[:(n_neg-n_pos)]]), axis=0)\n",
    "            y_balance = np.concatenate((y_train, [y_train[val[0]] for val in y_train_prob_P[:(n_neg-n_pos)]]), axis=0)\n",
    "        elif n_pos > n_neg:\n",
    "            X_balance = np.concatenate((X_train,[X_train[val[0]] for val in y_train_prob_N[:(n_pos-n_neg)]]), axis=0)\n",
    "            y_balance = np.concatenate((y_train,[y_train[val[0]] for val in y_train_prob_N[:(n_pos-n_neg)]]), axis=0)\n",
    "        \n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "    \n",
    "def S2T_p4_adj_blc_dm_pm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = 30\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    target_pred_prob_store = []\n",
    "    \n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        \n",
    "        target_pred = lr_clf.predict(test_features)\n",
    "        target_prob = lr_clf.predict_proba(test_features)[:, 0]\n",
    "        target_prob = abs(target_prob-0.5)\n",
    "        target_pred_prob_store.append([(target_pred[i],target_prob[i]) for i in range(len(test_labels))])\n",
    "        \n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        \n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1])\n",
    "        train_not_keep_index_P = [val[0] for val in y_train_prob_P[:len(y_prob_P)]]\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index_N = [val[0] for val in y_train_prob_N[:len(y_prob_N)]]\n",
    "        train_not_keep_index = train_not_keep_index_P + train_not_keep_index_N\n",
    "        \n",
    "        X_train = [X_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]        \n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    \n",
    "    max_prob = target_pred_prob_store[0]\n",
    "    for row in target_pred_prob_store:\n",
    "        max_prob = [sorted([max_prob[i],row[i]], key=lambda x: x[1], reverse = True)[0] for i in range(len(test_labels))]\n",
    "    max_pred = [val[0] for val in max_prob]\n",
    "    output_score = [max_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "    \n",
    "def S2T_p4_adj_blc_dm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1])\n",
    "        train_not_keep_index_P = [val[0] for val in y_train_prob_P[:len(y_prob_P)]]\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index_N = [val[0] for val in y_train_prob_N[:len(y_prob_N)]]\n",
    "        train_not_keep_index = train_not_keep_index_P + train_not_keep_index_N\n",
    "        X_train = [X_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]        \n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression(n_jobs = -1)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p4_adj_blc_dl(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index_P = [val[0] for val in y_train_prob_P[:len(y_prob_P)]]\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1])\n",
    "        train_not_keep_index_N = [val[0] for val in y_train_prob_N[:len(y_prob_N)]]\n",
    "        train_not_keep_index = train_not_keep_index_P + train_not_keep_index_N\n",
    "        X_train = [X_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]        \n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression(n_jobs = -1)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score   \n",
    "\n",
    "    \n",
    "def S2T_p4_adj_blc_int(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = 30\n",
    "    \n",
    "    s_center_P = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 1],axis = 0)\n",
    "    s_center_N = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 0],axis = 0)\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "        \n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        \n",
    "        X_test_keep_P = [X_test[i] for i in y_prob_P]\n",
    "        y_pred_keep_P = [y_pred[i] for i in y_prob_P]\n",
    "        X_test_keep_N = [X_test[i] for i in y_prob_N]\n",
    "        y_pred_keep_N = [y_pred[i] for i in y_prob_N]\n",
    "        \n",
    "        X_inter_P = [np.mean([s_center_P,val],axis = 0) for val in X_test_keep_P]\n",
    "        X_inter_N = [np.mean([s_center_N,val],axis = 0) for val in X_test_keep_N]\n",
    "        \n",
    "        X_train = np.concatenate([x for x in [X_train, X_test_keep, X_inter_P, X_inter_N] if len(x) >0], axis=0)\n",
    "        y_train = np.concatenate([x for x in [y_train, y_pred_keep,y_pred_keep_P,y_pred_keep_N] if len(x) >0], axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "\n",
    "def S2T_adj_blc_int(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = 30\n",
    "    \n",
    "    s_center_P = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 1],axis = 0)\n",
    "    s_center_N = np.mean([train_features[i] for i,value in enumerate(train_labels) if value == 0],axis = 0)\n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "        \n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        \n",
    "        X_test_keep_P = [X_test[i] for i in y_prob_P]\n",
    "        y_pred_keep_P = [y_pred[i] for i in y_prob_P]\n",
    "        X_test_keep_N = [X_test[i] for i in y_prob_N]\n",
    "        y_pred_keep_N = [y_pred[i] for i in y_prob_N]\n",
    "        \n",
    "        X_inter_P = [np.mean([s_center_P,val],axis = 0) for val in X_test_keep_P]\n",
    "        X_inter_N = [np.mean([s_center_N,val],axis = 0) for val in X_test_keep_N]\n",
    "        \n",
    "        X_train = np.concatenate([x for x in [X_train, X_inter_P, X_inter_N] if len(x) >0], axis=0)\n",
    "        y_train = np.concatenate([x for x in [y_train, y_pred_keep_P,y_pred_keep_N] if len(x) >0], axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "\n",
    "def S2T_al_p4_adj_blc(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = 30\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    difference = source_center - target_center\n",
    "    test_features = [val+difference for val in test_features]\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "\n",
    "def S2T_al_p4_adj_blc_dm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    difference = source_center - target_center\n",
    "    test_features = [val+difference for val in test_features]\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = [val[0] for val in y_prob_P[:top_n]]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = [val[0] for val in y_prob_N[:top_n]]\n",
    "        keep_index = y_prob_P + y_prob_N\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1])\n",
    "        train_not_keep_index_P = [val[0] for val in y_train_prob_P[:len(y_prob_P)]]\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index_N = [val[0] for val in y_train_prob_N[:len(y_prob_N)]]\n",
    "        train_not_keep_index = train_not_keep_index_P + train_not_keep_index_N\n",
    "        X_train = [X_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]        \n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'accuracy',accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),\n",
    "              'train_size',len(y_train))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression(n_jobs = -1)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p4_adj_Xblc(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i*2\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [abs(val-0.5) for val in y_prob]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob = sorted(y_prob, key=lambda x: x[1],reverse = True)\n",
    "        keep_index = [val[0] for val in y_prob[:top_n]]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p4_adj_Xblc_dm(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i*2\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [abs(val-0.5) for val in y_prob]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob = sorted(y_prob, key=lambda x: x[1],reverse = True)\n",
    "        keep_index = [val[0] for val in y_prob[:top_n]]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo += X_test_keep\n",
    "        y_pseudo += y_pred_keep\n",
    "        lr_clf = LogisticRegression(n_jobs = -1)\n",
    "        lr_clf.fit(X_pseudo, y_pseudo)\n",
    "        y_train_prob = lr_clf.predict_proba(X_train)[:, 0]\n",
    "        y_train_prob = [(i,val) for i,val in enumerate(y_train_prob)]\n",
    "        y_train_prob_P = [val for val in y_train_prob if val[1]<0.5]\n",
    "        y_train_prob_P = sorted(y_train_prob_P, key=lambda x: x[1])\n",
    "        train_not_keep_index_P = [val[0] for val in y_train_prob_P[:int(len(keep_index)/2)]]\n",
    "        y_train_prob_N = [val for val in y_train_prob if val[1]>=0.5]\n",
    "        y_train_prob_N = sorted(y_train_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        train_not_keep_index_N = [val[0] for val in y_train_prob_N[:int(len(keep_index)/2)]]\n",
    "        \n",
    "        \n",
    "        train_not_keep_index = train_not_keep_index_P + train_not_keep_index_N\n",
    "        X_train = [X_train[i] for i in range(len(X_train)) if i not in train_not_keep_index]        \n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = [y_train[i] for i in range(len(y_train)) if i not in train_not_keep_index]\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'pred_true',sum(y_pred_keep),'true_true',sum([y_test[i] for i in keep_index]))\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression(n_jobs = -1)\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_cutAndGo(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    p_threshold = 0.05\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        if (max([val[1] for val in y_prob_P]) < p_threshold) and (min([val[1] for val in y_prob_N])>(1-p_threshold)):\n",
    "            keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "            not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "            if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "                raise ValueError('top_n error!')\n",
    "\n",
    "            X_test_keep = [X_test[i] for i in keep_index]\n",
    "            y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "            y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i in keep_index]\n",
    "            print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "            X_test = [X_test[i] for i in not_keep_index]\n",
    "            y_test = [y_test[i] for i in not_keep_index]\n",
    "            if X_test == previous_r_target:\n",
    "                break\n",
    "            previous_r_target = X_test[:]\n",
    "        else:\n",
    "            X_train = X_pseudo[:]\n",
    "            y_train = y_pseudo[:]\n",
    "            break\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, 0, distribution\n",
    "    else:\n",
    "        return lm_score, 0\n",
    "    \n",
    "def S2T_al_cutAndGo(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    p_threshold = 0.05\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    difference = source_center - target_center\n",
    "    test_features = [val+difference for val in test_features]\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        if (max([val[1] for val in y_prob_P]) < p_threshold) and (min([val[1] for val in y_prob_N])>(1-p_threshold)):\n",
    "            keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "            not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "            if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "                raise ValueError('top_n error!')\n",
    "\n",
    "            X_test_keep = [X_test[i] for i in keep_index]\n",
    "            y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "            y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i in keep_index]\n",
    "            print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "            X_test = [X_test[i] for i in not_keep_index]\n",
    "            y_test = [y_test[i] for i in not_keep_index]\n",
    "            if X_test == previous_r_target:\n",
    "                break\n",
    "            previous_r_target = X_test[:]\n",
    "        else:\n",
    "            X_train = X_pseudo[:]\n",
    "            y_train = y_pseudo[:]\n",
    "            break\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, 0, distribution\n",
    "    else:\n",
    "        return lm_score, 0\n",
    "    \n",
    "    \n",
    "def S2T_p4_adj_blc_tSNE(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    fig_list = []\n",
    "    \n",
    "    s_P = [val for i,val in enumerate(train_features) if train_labels[i] == 1]\n",
    "    s_N = [val for i,val in enumerate(train_features) if train_labels[i] == 0]\n",
    "    s_P_c = np.mean(s_P,axis = 0)\n",
    "    s_N_c = np.mean(s_N,axis = 0)\n",
    "    s_c = np.mean(s_P + s_N,axis = 0)\n",
    "    t_P = [val for i,val in enumerate(X_test) if y_test[i] == 1]\n",
    "    t_N = [val for i,val in enumerate(X_test) if y_test[i] == 0]\n",
    "    t_P_c = np.mean(t_P,axis = 0)\n",
    "    t_N_c = np.mean(t_N,axis = 0)\n",
    "    t_c = np.mean(t_P + t_N,axis = 0)\n",
    "    \n",
    "    \n",
    "    feature = s_P + s_N + t_P + t_N + [s_P_c,s_N_c,t_P_c,t_N_c,s_c,t_c]\n",
    "\n",
    "    model = TSNE(learning_rate=200)\n",
    "    transformed = model.fit_transform(feature)\n",
    "\n",
    "    xs = transformed[:,0]\n",
    "    ys = transformed[:,1]\n",
    "\n",
    "    group = np.array([1]*len(s_P)+[2]*len(s_N)+[3]*len(t_P)+[4]*len(t_N)+[5,6,7,8,9,10])\n",
    "    cdict = {1: 'red', 2: 'blue', 3: 'green', 4: 'yellow', 5:'orange', 6:'purple', 7:'black', 8:'cyan',9:'green',10:'red'}\n",
    "\n",
    "    fig, ax = plt.subplots(figsize = [8,8])\n",
    "    for g in np.unique(group):\n",
    "        ix = np.where(group == g)\n",
    "        if g >=5:\n",
    "            ax.scatter(xs[ix], ys[ix], c = cdict[g], label = g, s = 100)\n",
    "        else:\n",
    "            ax.scatter(xs[ix], ys[ix], c = cdict[g], label = g, s = 10)\n",
    "    fig_list.append(fig)\n",
    "        \n",
    "        \n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "        y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "        \n",
    "        \n",
    "        t_P = [val for i,val in enumerate(X_test) if y_test[i] == 1]\n",
    "        t_N = [val for i,val in enumerate(X_test) if y_test[i] == 0]\n",
    "        \n",
    "        p_P = [val for i,val in enumerate(X_pseudo) if y_pseudo[i] == 1]\n",
    "        p_N = [val for i,val in enumerate(X_pseudo) if y_pseudo[i] == 0]\n",
    "        p_P_c = np.mean(p_P,axis = 0)\n",
    "        p_N_c = np.mean(p_N,axis = 0)\n",
    "        p_c = np.mean(p_P + p_N,axis = 0)\n",
    "        \n",
    "        feature = s_P + s_N + t_P + t_N + p_P + p_N + [\n",
    "            s_P_c, s_N_c,\n",
    "            t_P_c, t_N_c,\n",
    "            p_P_c ,p_N_c,\n",
    "            s_c, t_c, p_c]\n",
    "\n",
    "        model = TSNE(learning_rate=200)\n",
    "        transformed = model.fit_transform(feature)\n",
    "\n",
    "        xs = transformed[:,0]\n",
    "        ys = transformed[:,1]\n",
    "\n",
    "        group = np.array([1]*len(s_P)+[2]*len(s_N)+[3]*len(t_P)+[4]*len(t_N)+[5]*len(p_P)+[6]*len(p_N)+[7,8,9,10,11,12,13,14,15])\n",
    "        cdict = {1: 'red', 2: 'blue', 3: 'green', 4: 'yellow', 5:'pink', 6:'purple',\n",
    "                 7:'black', 8:'purple', \n",
    "                 9:'black', 10:'cyan',\n",
    "                 11:'brown',12:'magenta',\n",
    "                 13:'green',14:'red',15:'olive'}\n",
    "\n",
    "        fig, ax = plt.subplots(figsize = [8,8])\n",
    "        for g in np.unique(group):\n",
    "            ix = np.where(group == g)\n",
    "            if g >=7:\n",
    "                ax.scatter(xs[ix], ys[ix], c = cdict[g], label = g, s = 100)\n",
    "            else:\n",
    "                ax.scatter(xs[ix], ys[ix], c = cdict[g], label = g, s = 10)\n",
    "        fig_list.append(fig)\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, fig_list, distribution\n",
    "    else:\n",
    "        return lm_score, fig_list\n",
    "    \n",
    "def S2T_al_mc(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # moving center\n",
    "    top_n = num_i\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    difference = source_center - target_center\n",
    "    test_features = [val+difference for val in test_features]\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    \n",
    "    \n",
    "    s_P = [val for i,val in enumerate(X_train) if y_train[i] == 1]\n",
    "    s_N = [val for i,val in enumerate(X_train) if y_train[i] == 0]\n",
    "    s_P_c = np.mean(s_P,axis = 0)\n",
    "    s_N_c = np.mean(s_N,axis = 0)\n",
    "    s_c = np.mean(s_P + s_N,axis = 0)\n",
    "    p_P_c = s_P_c[:]\n",
    "    p_N_c = s_N_c[:]\n",
    "    \n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_pseudo_P = [(i,val,cos_dist(p_P_c,val)) for i,val in enumerate(X_test) if y_pred[i] == 0]\n",
    "        X_pseudo_P = sorted(X_pseudo_P, key=lambda x: x[2])\n",
    "        X_pseudo_P = X_pseudo_P[:top_n]\n",
    "        X_pseudo_N = [(i,val,cos_dist(p_N_c,val)) for i,val in enumerate(X_test) if y_pred[i] == 1]\n",
    "        X_pseudo_N = sorted(X_pseudo_N, key=lambda x: x[2])\n",
    "        X_pseudo_N = X_pseudo_N[:top_n]\n",
    "\n",
    "        keep_index = [val[0] for val in X_pseudo_P] + [val[0] for val in X_pseudo_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "        y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "        p_P_c =  np.mean([X_pseudo[i] for i,val in enumerate(y_pseudo) if val == 0],axis = 0)\n",
    "        p_N_c =  np.mean([X_pseudo[i] for i,val in enumerate(y_pseudo) if val == 1],axis = 0)\n",
    "        \n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "              'true_true',sum([y_test[i] for i in keep_index])\n",
    "             )\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "        \n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)       \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_p_align(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    difference = source_center - target_center\n",
    "    test_features = [val+difference for val in test_features]\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    \n",
    "    s_P = [val for i,val in enumerate(X_train) if y_train[i] == 1]\n",
    "    s_N = [val for i,val in enumerate(X_train) if y_train[i] == 0]\n",
    "    s_P_c = np.mean(s_P,axis = 0)\n",
    "    s_N_c = np.mean(s_N,axis = 0)\n",
    "    s_c = np.mean(s_P + s_N,axis = 0)\n",
    "    \n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "#         X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "#         y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "        y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        try:\n",
    "            print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "        except:\n",
    "            pass\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        \n",
    "        if len(X_test)>0:\n",
    "            t_P = [val for i,val in enumerate(X_pseudo) if y_pseudo[i] == 1]\n",
    "            t_N = [val for i,val in enumerate(X_pseudo) if y_pseudo[i] == 0]\n",
    "            t_P_c = np.mean(t_P,axis = 0)\n",
    "            t_N_c = np.mean(t_N,axis = 0)\n",
    "            t_c = np.mean(np.concatenate([data for data in [t_P, t_N] if len(data)>0], axis=0),axis = 0)\n",
    "            reg = LinearRegression().fit([t_P_c,t_N_c], [s_P_c,s_N_c])\n",
    "            X_test = reg.predict(X_test)\n",
    "            X_pseudo = reg.predict(X_pseudo)\n",
    "        \n",
    "        \n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "    \n",
    "def S2T_al_mc_align(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # moving center\n",
    "    top_n = num_i\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    difference = source_center - target_center\n",
    "    test_features = [val+difference for val in test_features]\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    \n",
    "    \n",
    "    s_P = [val for i,val in enumerate(X_train) if y_train[i] == 1]\n",
    "    s_N = [val for i,val in enumerate(X_train) if y_train[i] == 0]\n",
    "    s_P_c = np.mean(s_P,axis = 0)\n",
    "    s_N_c = np.mean(s_N,axis = 0)\n",
    "    s_c = np.mean(s_P + s_N,axis = 0)\n",
    "    \n",
    "#     lr_clf = LogisticRegression()\n",
    "#     lr_clf.fit(X_train, y_train)\n",
    "#     y_pred = lr_clf.predict(X_test)\n",
    "#     y_pred = lr_clf.predict(X_test)\n",
    "#     y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "#     y_prob = [(i,val) for i,val in enumerate(y_prob)]\n",
    "#     y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "#     y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "#     x_prob_P = [X_test[i] for i,val in y_prob_P[:top_n]]\n",
    "#     y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "#     y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "#     x_prob_N = [X_test[i] for i,val in y_prob_N[:top_n]]\n",
    "    \n",
    "#     p_P_c =  np.mean(x_prob_P,axis = 0)\n",
    "#     p_N_c =  np.mean(x_prob_N,axis = 0)\n",
    "    \n",
    "    p_P_c = s_P_c[:]\n",
    "    p_N_c = s_N_c[:]\n",
    "    \n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_pseudo_P = [(i,val,cos_dist(p_P_c,val)) for i,val in enumerate(X_test) if y_pred[i] == 0]\n",
    "        X_pseudo_P = sorted(X_pseudo_P, key=lambda x: x[2])\n",
    "        X_pseudo_P = X_pseudo_P[:top_n]\n",
    "        X_pseudo_N = [(i,val,cos_dist(p_N_c,val)) for i,val in enumerate(X_test) if y_pred[i] == 1]\n",
    "        X_pseudo_N = sorted(X_pseudo_N, key=lambda x: x[2])\n",
    "        X_pseudo_N = X_pseudo_N[:top_n]\n",
    "\n",
    "        keep_index = [val[0] for val in X_pseudo_P] + [val[0] for val in X_pseudo_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "        y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "        p_P_c =  np.mean([X_pseudo[i] for i,val in enumerate(y_pseudo) if val == 0],axis = 0)\n",
    "        p_N_c =  np.mean([X_pseudo[i] for i,val in enumerate(y_pseudo) if val == 1],axis = 0)\n",
    "        \n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "              'true_true',sum([y_test[i] for i in keep_index])\n",
    "             )\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "         \n",
    "        if len(X_test)>0:\n",
    "            t_P = [val for i,val in enumerate(X_pseudo) if y_pseudo[i] == 1]\n",
    "            t_N = [val for i,val in enumerate(X_pseudo) if y_pseudo[i] == 0]\n",
    "            t_P_c = np.mean(t_P,axis = 0)\n",
    "            t_N_c = np.mean(t_N,axis = 0)\n",
    "            t_c = np.mean(np.concatenate([data for data in [t_P, t_N] if len(data)>0], axis=0),axis = 0)\n",
    "            reg = LinearRegression().fit([t_P_c,t_N_c], [s_P_c,s_N_c])\n",
    "            X_test = reg.predict(X_test)\n",
    "            X_pseudo = reg.predict(X_pseudo)\n",
    "            X_test_keep = reg.predict(X_test_keep)\n",
    "#             X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "#             y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)       \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_hh(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    test_features = hh(train_features, train_labels, test_features, test_labels)\n",
    "   \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    \n",
    "    return original_score, lr_clf.score(test_features, test_labels)\n",
    "\n",
    "def linear_trans(train_features, train_labels, test_features, test_labels):\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    s_P = [val for i,val in enumerate(train_features) if train_labels[i] == 1]\n",
    "    s_N = [val for i,val in enumerate(train_features) if train_labels[i] == 0]\n",
    "    s_P_c = np.mean(s_P,axis = 0)\n",
    "    s_N_c = np.mean(s_N,axis = 0)\n",
    "    s_c = np.mean(s_P + s_N,axis = 0)\n",
    "    \n",
    "    t_P = [val for i,val in enumerate(test_features) if test_labels[i] == 1]\n",
    "    t_N = [val for i,val in enumerate(test_features) if test_labels[i] == 0]\n",
    "    t_P_c = np.mean(t_P,axis = 0)\n",
    "    t_N_c = np.mean(t_N,axis = 0)\n",
    "    t_c = np.mean(t_P + t_N,axis = 0)\n",
    "    \n",
    "    v_s = s_P_c - s_N_c\n",
    "    v_t = t_P_c - t_N_c\n",
    "    \n",
    "    reg = LinearRegression().fit([t_P_c,t_N_c], [s_P_c,s_N_c])\n",
    "    test_features = reg.predict(test_features)\n",
    "    return test_features\n",
    "\n",
    "def S2T_p4_adj_blc_hh(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    \n",
    "    test_features = linear_trans(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        try:\n",
    "            print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "        except:\n",
    "            pass\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score, gradual_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_al_mc_hh(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    # moving center\n",
    "    top_n = num_i\n",
    "    \n",
    "    test_features = hh(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    \n",
    "    \n",
    "    s_P = [val for i,val in enumerate(X_train) if y_train[i] == 1]\n",
    "    s_N = [val for i,val in enumerate(X_train) if y_train[i] == 0]\n",
    "    s_P_c = np.mean(s_P,axis = 0)\n",
    "    s_N_c = np.mean(s_N,axis = 0)\n",
    "    s_c = np.mean(s_P + s_N,axis = 0)\n",
    "    p_P = [val for i,val in enumerate(X_test) if y_test[i] == 1]\n",
    "    p_N = [val for i,val in enumerate(X_test) if y_test[i] == 0]\n",
    "    p_P_c = np.mean(p_P,axis = 0)\n",
    "    p_N_c = np.mean(p_N,axis = 0)    \n",
    "    \n",
    "    \n",
    "#     p_P_c = p_P_c[:]\n",
    "#     p_N_c = s_N_c[:]\n",
    "    \n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        X_pseudo_P = [(i,val,cos_dist(p_P_c,val)) for i,val in enumerate(X_test) if y_pred[i] == 0]\n",
    "        X_pseudo_P = sorted(X_pseudo_P, key=lambda x: x[2])\n",
    "        X_pseudo_P = X_pseudo_P[:top_n]\n",
    "        X_pseudo_N = [(i,val,cos_dist(p_N_c,val)) for i,val in enumerate(X_test) if y_pred[i] == 1]\n",
    "        X_pseudo_N = sorted(X_pseudo_N, key=lambda x: x[2])\n",
    "        X_pseudo_N = X_pseudo_N[:top_n]\n",
    "\n",
    "        keep_index = [val[0] for val in X_pseudo_P] + [val[0] for val in X_pseudo_N]\n",
    "        not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "        if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "            raise ValueError('top_n error!')\n",
    "\n",
    "        X_test_keep = [X_test[i] for i in keep_index]\n",
    "        y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "        X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "        y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "        X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "        y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "        p_P_c =  np.mean([X_pseudo[i] for i,val in enumerate(y_pseudo) if val == 0],axis = 0)\n",
    "        p_N_c =  np.mean([X_pseudo[i] for i,val in enumerate(y_pseudo) if val == 1],axis = 0)\n",
    "        \n",
    "        y_pred_store += y_pred_keep\n",
    "        y_test_store += [y_test[i] for i in keep_index]\n",
    "        print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "              'true_true',sum([y_test[i] for i in keep_index])\n",
    "             )\n",
    "        X_test = [X_test[i] for i in not_keep_index]\n",
    "        y_test = [y_test[i] for i in not_keep_index]\n",
    "        if X_test == previous_r_target:\n",
    "            break\n",
    "        previous_r_target = X_test[:]\n",
    "        \n",
    "    if len(y_pred_store) != len(test_labels):\n",
    "        raise ValueError('output dimension error!')\n",
    "    output_score = [y_pred_store[i]==y_test_store[i] for i in range(len(y_test_store))]\n",
    "    gradual_score = sum(output_score)/len(output_score)       \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, gradual_score, distribution\n",
    "    else:\n",
    "        return lm_score, gradual_score\n",
    "    \n",
    "def S2T_hh_cutAndGo(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    top_n = num_i\n",
    "    p_threshold = 0.10\n",
    "    \n",
    "    source_center = np.mean(train_features,axis = 0)\n",
    "    target_center = np.mean(test_features,axis = 0)\n",
    "    test_features = hh(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    # gradual training\n",
    "    X_train = train_features[:]\n",
    "    y_train = train_labels[:]\n",
    "    X_test = test_features[:]\n",
    "    y_test = test_labels[:]\n",
    "    y_pred_store = []\n",
    "    y_test_store = []\n",
    "    previous_r_target = []\n",
    "    X_pseudo = []\n",
    "    y_pseudo = []\n",
    "    while len(X_test)>0:\n",
    "        lr_clf = LogisticRegression()\n",
    "        lr_clf.fit(X_train, y_train)\n",
    "        y_pred = lr_clf.predict(X_test)\n",
    "        y_prob = lr_clf.predict_proba(X_test)[:, 0]\n",
    "        y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "        y_prob_P = [val for val in y_prob if val[1]<0.5]\n",
    "        y_prob_P = sorted(y_prob_P, key=lambda x: x[1])\n",
    "        y_prob_P = y_prob_P[:top_n]\n",
    "        y_prob_N = [val for val in y_prob if val[1]>=0.5]\n",
    "        y_prob_N = sorted(y_prob_N, key=lambda x: x[1],reverse = True)\n",
    "        y_prob_N = y_prob_N[:top_n]\n",
    "        if (max([val[1] for val in y_prob_P]) < p_threshold) and (min([val[1] for val in y_prob_N])>(1-p_threshold)):\n",
    "            keep_index = [val[0] for val in y_prob_P] + [val[0] for val in y_prob_N]\n",
    "            not_keep_index = [i for i in range(len(y_pred)) if i not in keep_index]\n",
    "            if len(keep_index)+len(not_keep_index) != len(y_pred):\n",
    "                raise ValueError('top_n error!')\n",
    "\n",
    "            X_test_keep = [X_test[i] for i in keep_index]\n",
    "            y_pred_keep = [y_pred[i] for i in keep_index]\n",
    "            X_train = np.concatenate((X_train, X_test_keep), axis=0)\n",
    "            y_train = np.concatenate((y_train, y_pred_keep), axis=0)\n",
    "            X_pseudo = np.concatenate([data for data in [X_pseudo, X_test_keep] if len(data)>0], axis=0)\n",
    "            y_pseudo = np.concatenate([data for data in [y_pseudo, y_pred_keep] if len(data)>0], axis=0)\n",
    "            y_pred_store += y_pred_keep\n",
    "            y_test_store += [y_test[i] for i in keep_index]\n",
    "            print('total:',len(y_pred_keep),'accuracy',round(accuracy_score(y_pred_keep,[y_test[i] for i in keep_index]),2),\n",
    "                  'true_true',sum([y_test[i] for i in keep_index]),\n",
    "                  'min_P',round(max([val[1] for val in y_prob_P]),2),\n",
    "                  'min_N',round(min([val[1] for val in y_prob_N]),2),\n",
    "                 )\n",
    "            X_test = [X_test[i] for i in not_keep_index]\n",
    "            y_test = [y_test[i] for i in not_keep_index]\n",
    "            if X_test == previous_r_target:\n",
    "                break\n",
    "            previous_r_target = X_test[:]\n",
    "        else:\n",
    "            X_train = X_pseudo[:]\n",
    "            y_train = y_pseudo[:]\n",
    "            break\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(X_train, y_train)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    lm_score = [y_pred[i]==test_labels[i] for i in range(len(test_labels))]\n",
    "    lm_score = sum(lm_score)/len(lm_score)\n",
    "    print(lm_score)\n",
    "    \n",
    "    if dist_eval:\n",
    "        return lm_score, 0, distribution\n",
    "    else:\n",
    "        return lm_score, 0\n",
    "    \n",
    "def S2T_hh(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    \n",
    "    sourcePos = [val for i,val in enumerate(train_features) if train_labels[i] == 1]\n",
    "    sourceNeg = [val for i,val in enumerate(train_features) if train_labels[i] == 0]\n",
    "    targetPos = [val for i,val in enumerate(test_features) if test_labels[i] == 1]\n",
    "    targetNeg = [val for i,val in enumerate(test_features) if test_labels[i] == 0]\n",
    "    v = np.mean(sourcePos, axis=0) - np.mean(sourceNeg, axis=0)\n",
    "    u = np.mean(targetPos, axis=0) - np.mean(targetNeg, axis=0)\n",
    "    c1 = np.mean(np.concatenate([targetPos, targetNeg], axis=0), axis=0)\n",
    "    c2 = np.mean(np.concatenate([sourcePos, sourceNeg], axis=0), axis=0)\n",
    "\n",
    "    test_features = hh(u, v, c1, c2, test_features)\n",
    "   \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    \n",
    "    return original_score, lr_clf.score(test_features, test_labels)\n",
    "\n",
    "def S2T_p_hh(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    threshold = 0.05\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    y_prob = lr_clf.predict_proba(test_features)[:, 0]\n",
    "    y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "    y_prob_P = [val for val in y_prob if val[1]<threshold]\n",
    "    y_prob_N = [val for val in y_prob if val[1]>=(1-threshold)]\n",
    "    \n",
    "    \n",
    "    sourcePos = [val for i,val in enumerate(train_features) if train_labels[i] == 1]\n",
    "    sourceNeg = [val for i,val in enumerate(train_features) if train_labels[i] == 0]\n",
    "    targetPos = [test_features[val[0]] for val in y_prob_P]\n",
    "    targetNeg = [test_features[val[0]] for val in y_prob_N]\n",
    "    v = np.mean(sourcePos, axis=0) - np.mean(sourceNeg, axis=0)\n",
    "    u = np.mean(targetPos, axis=0) - np.mean(targetNeg, axis=0)\n",
    "    c1 = np.mean(test_features, axis=0)\n",
    "    c2 = np.mean(np.concatenate([sourcePos, sourceNeg], axis=0), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_features = hh(u, v, c1, c2,test_features)\n",
    "   \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    \n",
    "    return original_score, lr_clf.score(test_features, test_labels)\n",
    "\n",
    "\n",
    "def hh(u, v, c1, c2,points):\n",
    "    u_mag = np.linalg.norm(u)\n",
    "    u_unit = u / u_mag\n",
    "    \n",
    "    v_mag = np.linalg.norm(v)\n",
    "    v_unit = v / v_mag\n",
    "\n",
    "    #Scaling so pos-neg vectors have the same magnitude\n",
    "    scaled_points = points * v_mag / u_mag\n",
    "    scaled_c1 = c1 * v_mag / u_mag\n",
    "    \n",
    "    #gettinng dimension of vector space\n",
    "    k = len(c2)\n",
    "\n",
    "    #calculating isometric linear transformation: householder transformation\n",
    "    A = np.eye(k) - (2 * (np.outer(u_unit-v_unit, u_unit-v_unit)\\\n",
    "                        /  np.inner(u_unit-v_unit, u_unit-v_unit)))\n",
    "\n",
    "    #applying isometric transformation\n",
    "    points_after_isometric = scaled_points @ A.T\n",
    "    c1_after_isometric = scaled_c1 @ A.T\n",
    "\n",
    "    #translation\n",
    "    points_after_translation = points_after_isometric + (c2 - c1_after_isometric)\n",
    "\n",
    "    return points_after_translation\n",
    "\n",
    "\n",
    "def S2T_p_hh_selftrain(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    threshold = 0.05\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    y_prob = lr_clf.predict_proba(test_features)[:, 0]\n",
    "    y_prob = [(i,val,y_pred[i]) for i,val in enumerate(y_prob)]\n",
    "    y_prob_P = [val for val in y_prob if val[1]<threshold]\n",
    "    y_prob_N = [val for val in y_prob if val[1]>=(1-threshold)]\n",
    "    \n",
    "    \n",
    "    sourcePos = [val for i,val in enumerate(train_features) if train_labels[i] == 1]\n",
    "    sourceNeg = [val for i,val in enumerate(train_features) if train_labels[i] == 0]\n",
    "    targetPos = [test_features[val[0]] for val in y_prob_P]\n",
    "    targetNeg = [test_features[val[0]] for val in y_prob_N]\n",
    "    v = np.mean(sourcePos, axis=0) - np.mean(sourceNeg, axis=0)\n",
    "    u = np.mean(targetPos, axis=0) - np.mean(targetNeg, axis=0)\n",
    "    c1 = np.mean(test_features, axis=0)\n",
    "    c2 = np.mean(np.concatenate([sourcePos, sourceNeg], axis=0), axis=0)\n",
    "    \n",
    "    \n",
    "    \n",
    "    test_features = hh(u, v, c1, c2,test_features)\n",
    "   \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    \n",
    "    train_features = np.concatenate([train_features, test_features], axis=0)\n",
    "    train_labels = np.concatenate([train_labels, y_pred], axis=0)\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    \n",
    "    \n",
    "    return original_score, lr_clf.score(test_features, test_labels)\n",
    "\n",
    "def S2T_selftrain(train_features, train_labels, test_features, test_labels, num_i, eval_bool = False, dist_eval = False):\n",
    "    original_score = S2T(train_features, train_labels, test_features, test_labels)\n",
    "       \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    y_pred = lr_clf.predict(test_features)\n",
    "    \n",
    "    train_features = np.concatenate([train_features, test_features], axis=0)\n",
    "    train_labels = np.concatenate([train_labels, y_pred], axis=0)\n",
    "    \n",
    "    lr_clf = LogisticRegression()\n",
    "    lr_clf.fit(train_features, train_labels)\n",
    "    \n",
    "    \n",
    "    return original_score, lr_clf.score(test_features, test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Books', 'Electronics', 'Home_and_Kitchen', 'Movies_and_TV']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# ## Save pickle\n",
    "# with open(\"./data/amazon_review/data.pickle\",\"wb\") as fw:\n",
    "#     pickle.dump(all_data, fw)\n",
    "\n",
    "## Load pickle\n",
    "with open(\"./data/amz_bekd_bert.pickle\",\"rb\") as fr:\n",
    "    all_data = pickle.load(fr)\n",
    "[val[2] for val in all_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "metadata": {},
   "outputs": [],
   "source": [
    "upperbounds = []\n",
    "for i in all_data:\n",
    "    lr_clf = LogisticRegression()\n",
    "    upperbounds.append(np.mean(cross_val_score(lr_clf,X = i[0], y = i[1], cv = 5)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9123749999999999"
      ]
     },
     "execution_count": 216,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(upperbound)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "S2T_p_hh_selftrain_result = all_combination_test(S2T_p_hh_selftrain,100, dist_eval = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n"
     ]
    }
   ],
   "source": [
    "S2T_selftrain_result = all_combination_test(S2T_selftrain,100, dist_eval = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data = [\n",
    "            S2T_hh_result[1],\n",
    "    S2T_selftrain_result[2],\n",
    "    S2T_hh_scale_result[2],\n",
    "    S2T_p_hh_scale_result[2],\n",
    "    S2T_p_hh_selftrain_result[2]\n",
    "#             upperbounds\n",
    "    \n",
    "            ]\n",
    "labels = [ 'lowerbound', \n",
    "          'S2T_selftrain',\n",
    "          'S2T_hh',\n",
    "          'S2T_p_hh',\n",
    "          'S2T_p_hh_selftrain'\n",
    "#           'upperbound'\n",
    "                     ]\n",
    "plot_mean = [np.mean(val) for val in plot_data]\n",
    "plot_stdev = [np.std(val) for val in plot_data]\n",
    "\n",
    "plt.figure(figsize = [6,8])\n",
    "plt.bar(labels,plot_mean,yerr=plot_stdev, capsize=9)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(rotation=45)\n",
    "plt.ylim([0.84, 0.92])\n",
    "plt.axhline(y = 0.9123749999999999,color = 'r') #np.mean(upperbounds[2])\n",
    "plt.axhline(y = 0.8797916666666666,color = 'r')#np.mean(si2ti_prob_4_5[1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 576x576 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_data = [S2T_dist_blc[1],\n",
    "             S2T_cutAndGo_result[1],\n",
    "             S2T_al_cutAndGo_result[1],\n",
    "             S2T_mc_result[1],\n",
    "             S2T_al_mc_result[1],\n",
    "             S2T_al_mc_align_result[1],\n",
    "             S2T_al_mc_align_result[2],\n",
    "             S2T_align_result[2],\n",
    "             S2T_p4_adj_blc_al_result[1],\n",
    "             S2T_al_mc_hh_result[1],\n",
    "              S2T_al_mc_hh_result[2],\n",
    "             S2T_al_cutAndGo_hh_result[1],\n",
    "             S2T_hh_cutAndGo_result[1],\n",
    "             S2T_hh_cutAndGo_result_001[1]\n",
    "            ]\n",
    "labels = [             'prev best', \n",
    "                      'cutAndGo',\n",
    "          'al_cutAndGo',\n",
    "          'moving center',\n",
    "          'moving center, allocated',\n",
    "          'S2T_al_mc_align_lm',\n",
    "          'S2T_al_mc_align_s',\n",
    "          'S2T_align_result',\n",
    "          'S2T_p4_adj_blc_al_lm',\n",
    "          ' S2T_al_mc_hh_lm',\n",
    "          ' S2T_al_mc_hh_s',\n",
    "          'S2T_hh_cutAndGo_0.05',\n",
    "          'S2T_hh_cutAndGo_0.025',\n",
    "          'S2T_hh_cutAndGo_0.01'\n",
    "                     ]\n",
    "plot_mean = [np.mean(val) for val in plot_data]\n",
    "plot_stdev = [np.std(val) for val in plot_data]\n",
    "\n",
    "plt.figure(figsize = [8,8])\n",
    "plt.bar(labels,plot_mean,yerr=plot_stdev, capsize=9)\n",
    "plt.ylabel('accuracy')\n",
    "plt.xticks(rotation=90)\n",
    "plt.ylim([0.86, 0.92])\n",
    "plt.axhline(y = 0.9123749999999999,color = 'r') #np.mean(upperbounds[2])\n",
    "plt.axhline(y = 0.8797916666666666,color = 'r')#np.mean(si2ti_prob_4_5[1])\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
